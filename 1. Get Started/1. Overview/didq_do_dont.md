# Do and Don’t in Data Ingestion and Data Quality(DIDQ)

* Any Engineer working on DIDQ should have basic knowledge of Technology Stack used in DIDQ.
* DataOps Team have training & recorded sessions for DIDQ, and that will be provided as a Mandatory Knowledge Sessions.
* DataOps Engineers will provide the details of DIDQ along the Built, any upgrade done by DataOps Engineers will be informed to all the Teams through Teams Channel/Mails and the upgraded code will be available in Git Developer Branch for 15 days and will be merged into Master after 15 days of time.
* All the Build Services are tested and available in the Provided Git Repository, if you change any code. Please validate the Services according to your requirement. DataOps Engineers will not be responsible.
* Master Services carries the arrangement of services and teams should be responsible of arranging the services as per there need, DataOps Engineers will help the teams through the required Knowledge Sessions and manual docs to learn this quickly.
* Azure Data Factory Pipelines is arrangement to Run/Restart DIDQ Automatically. If any upgrade is done, DataOps Team will not be responsible.
* Team can upgrade the service as per their need but DataOps Engineers will not be responsible after changes.
* No changes should be make in all the Metadata Tables given by DataOps Engineers, if any upgrade is done, DataOps Team will not be responsible for any concerns.
* Addition of any Metadata Table apart from DIDQ defined tables will not be considered as a part of DIDQ.
* Stage Rejected and Curated Rejected Delta Tables are Mandatory Rejected tables and if implementation Team don’t use these mandatory tables, they should bare the consciousness. DataOps Engineers will not be responsible.
